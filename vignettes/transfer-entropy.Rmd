---
title: "RTransferEntropy"
author: "Simon Behrendt, Thomas Dimpfl, Franziska J. Peter, David J. Zimmermann"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{RTransferEntropy}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  cache = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.height = 5
)
```

## Introduction to RTransferEntropy

The measurement of  information transfer between different time series is the basis of research questions in various research areas, including  biometrics, economics, ecological modelling, neuroscience, sociology, and thermodynamics. The quantification of information transfer commonly relies on measures that have been derived from the subject-specific assumptions and restrictions concerning the underlying stochastic processes or theoretical models. With the development of transfer entropy, information theory based measures have become a popular alternative to quantify information flows within various disciplines. Transfer entropy is a non-parametric measure of directed, time-asymmetric information transfer between two processes.

We show how to test for and quantify the information flow between two time series with Shannon transfer entropy and R\'{e}nyi transfer entropy using the package `RTransferEntropy`. Let us first have a brief look on the methodology, the bias correction applied to calculate effective transfer entropy and describe our approach to statistical inference. Afterwards, we introduce the package in detail and demonstrate its functionality in several applications to simulated processes as well as an application to financial time series.

## Measuring information flows using transfer entropy

Let $log$ denote the base 2 logarithm, then informational gain is measured in bits. Shannon entropy states that for a discrete variable $J$ with probability distribution $p(j)$, where j stands for the different outcomes the random variable $J$ can take, the average number of bits required to optimally encode independent draws from the distribution of J can be calculated as 

$$
  H_J = - \sum_j p(j) \times log \left(p(j)\right).
$$
Strictly speaking, Shannon's formula  is a measure for uncertainty, which increases with the number of bits needed to optimally encode a sequence of realizations of $J$. In order to measure information flow between two processes, Shannon entropy is combined with the concept of the Kullback-Leibler distance and by assuming the underlying processes evolve over time according to a Markov process. Let  $I$ and $J$ denote two discrete variables with marginal probability distributions $p(i)$ and $p(j)$, joint probability $p(i,j)$, whose dynamical structures correspond to stationary Markov processes of order $k$ (process $I$) and $l$ (process $J$). The Markov property implies that the probability to observe $I$ at time $t+1$ in state $i$ conditional on the $k$ previous observations is $p(i_{t+1}|i_t,...,i_{t-k+1})=p(i_{t+1}|i_t,...,i_{t-k})$. The average number of bits needed to encode the observation in $t+1$ if the previous $k$ values are known is given by

$$
  h_I(k)=- \sum_i p\left(i_{t+1}, i_t^{(k)}\right) \times log \left(p\left(i_{t+1}|i_t^{(k)}\right)\right),
$$
where $i^{(k)}_t=(i_t,...,i_{t-k+1})$ (analogously for process $J$). In the bivariate case, information flow from process $J$ to process $I$ is measured by quantifying the deviation from the generalized Markov property $p(i_{t+1}| i_t^{(k)})=p(i_{t+1}| i_t^{(k)},j_t^{(l)})$ relying on the Kullback-Leibler distance. Thus, (Shannon) transfer entropy is given by 

$$
  T_{J \rightarrow I}(k,l) = \sum_{i,j} p\left(i_{t+1}, i_t^{(k)}, j_t^{(l)}\right) \times log \left(\frac{p\left(i_{t+1}| i_t^{(k)}, j_t^{(l)}\right)}{p\left(i_{t+1}|i_t^{(k)}\right)}\right),
$$
where $T_{J\rightarrow I}$ consequently measures the information flow from $J$ to $I$ ( $T_{I \rightarrow J}$ as a measure for the information flow from $I$ to $J$ can be derived analogously). Transfer entropy can also be based on R\'{e}nyi entropy rather than Shannon entropy, which depends on a weighting parameter $q$ and can be calculated as

$$
  H^q_J = \frac{1}{1-q} log \left(\sum_j p^q(j)\right), 
$$
with  $q >0$. For $q\rightarrow 1$ R\'{e}nyi entropy converges to Shannon entropy. For $0<q<1$  tail events receive more weight, while for $q>1$ the weights on center observations dominate. Consequently, R\'{e}nyi entropy provides a more flexible tool for estimating uncertainty, since different areas of a distribution can be emphasized, depending on the parameter $q$. Using the escort distribution $\phi_q(j)=\frac{p^q(j)}{\sum_j p^q(j))}$ with $q >0$ to normalize the weighted distributions, the R\'{e}nyi transfer entropy measure is given by

$$
  RT_{J \rightarrow I}(k,l) = \frac{1}{1-q} log \left(\frac{\sum_i \phi_q\left(i_t^{(k)}\right)p^q\left(i_{t+1}|i^{(k)}_t\right)}{\sum_{i,j} \phi_q\left(i^{(k)}_t,j^{(l)}_t\right)p^q\left(i_{t+1}|i^{(k)},j^{(l)}_t \right)}\right).
$$
Analogously to (Shannon) transfer entropy R\'{e}nyi entropy  measures the information flow from $J$ to $I$.

These transfer entropy estimates  are commonly biased due to small sample effects. A remedy is provided by the effective transfer entropy, which is computed in the following way 

$$
  ET_{J \rightarrow I}(k,l):=  T_{J \rightarrow I}(k,l)- T_{J_{\text{shuffled}} \rightarrow I}(k,l),
$$
where $T_{J_{\text{shuffled}} \rightarrow I}(k,l)$ indicates the transfer entropy using a shuffled  version of the time series of $J$. Shuffling implies randomly drawing values from the time series of $J$ and realigning them to generate a new time series. This procedure destroys the time series dependencies of $J$ as well as the statistical dependencies between $J$ and $I$. As a result $T_{J_{\text{shuffled}} \rightarrow I}(k,l)$ converges to zero with increasing sample size and any nonzero value of $T_{J_{\text{shuffled}} \rightarrow I}(k,l)$ is due to small sample effects. The transfer entropy estimates from shuffled data can therefore be used as an estimator for the bias induced by these small sample effects. To derive a consistent estimator, shuffling is repeated many times and the average of the resulting shuffled transfer entropy estimates across all replications is subtracted from the Shannon or R\'{e}nyi transfer entropy estimate to obtain a bias corrected effective transfer entropy estimate.

In order to assess the statistical significance of transfer entropy estimates, we rely on  a Markov block bootstrap. In contrast to shuffling, the Markov block bootstrap retains the dependencies within each series. Thereby, it generates the distribution of transfer entropy estimates under the null hypothesis of no information transfer, i.e. randomly drawn blocks of process $J$ are realigned to form a simulated series, which retains the univariate dependencies of $J$ but eliminates the statistical dependencies between $J$ and $I$. Shannon or R\'{e}nyi transfer entropy is then estimated based on the simulated time series. Repeating this procedure yields the distribution of the transfer entropy estimate under the null of no information flow. The p-value associated with the null hypothesis of no information transfer is given by $1-\hat{q}_{TE}$, where $\hat{q}_{TE}$ denotes the quantile of the simulated distribution that corresponds to the original transfer entropy estimate.

The calculation of Shannon and R\'{e}nyi transfer entropy is based on discrete data. If the data does not exhibit a discrete structure that allows for transfer entropy estimation, it has to be discretized. This can be achieved by symbolic recoding, i.e. by partitioning the data into a finite number of bins, which can be based by either defining upper and lower bounds for the bins a priori or by choosing specific quantiles of the empirical distribution of the data. Denote the bounds specified for the $n$ bins by $q_1, q_2, ..., q_n$, where $q_1< q_2< ... <q_n$, and consider a time series  denoted by $y_t$,  the data is recoded as

$$
  S_t=
  \begin{cases}
  ~1~~~~~~~~ \mbox{ for }~  y_t\leq q_1\\
  ~ 2~ ~~~~~~~\mbox{ for }~  q_1<y_t<q_2\\
  ~\vdots~~~~~~~~~~~~~~~~~\vdots\\
  ~n-1~~\mbox{ for }~  q_{n-1}<y_t<q_n\\
  ~n ~~~~~~~~\mbox{     for } ~ y_t\geq q_n
  \end{cases}.
$$
Thereby, each value in the observed time series $y_t$ is replaced by an integer ($1$,$2$,...,$n$), according to how $S_t$ relates to the interval specified by the lower and upper bounds $q_1$ to $q_n$. The choice of the bins should be motivated by the distribution of the data.

## The `RTransferEntropy` package

Testing for and quantifying the information flow between two time series with Shannon and R\'{e}nyi transfer entropy, as outlined above, can now easily be implemented with the package `RTransferEntropy`. The package is installed and loaded in the usual way.

```{r, include=F}
  library(RTransferEntropy)
```

```{r, eval=F}
  install.packages('RTransferEntropy')
  library(RTransferEntropy)
```

The main function is `transfer_entropy()`, which creates an object of class `TEResult` that contains the respective transfer entropy estimates (both in $J \rightarrow I$ and $I \rightarrow J$ direction), the related effective transfer entropy estimates, standard errors and p-values for the estimated values, an indication of statistical significance, and quantiles of the bootstrap samples (if the number of bootstrap replications is specified to be greater than zero). Some sub-functions are written in `C++` to speed up computations and an option for parallel computing is available, allowing for more efficient programming when `transfer_entropy()` is called multiple times or fairly long time series make calculations of effective transfer entropies and bootstrap inference time consuming. Let us describe the usage of `transfer_entropy()` and its options.

```{r, eval=F}
transfer_entropy(x, y, lx = 1, ly = 1, q = 0.1, 
                 entropy = c('Shannon', 'Renyi'), shuffles = 100, 
                 cl = parallel::detectCores() - 1, 
                 type = c('quantiles', 'bins', 'limits'),
                 quantiles = c(5, 95), bins = NULL, limits = NULL,
                 nboot = 300, burn = 50, quiet = FALSE, seed = NULL)
```

The function takes the following arguments:

### Arguments

* `x`: a vector of numeric values, ordered by time.
* `y`: a vector of numeric values, ordered by time.
* `lx`: Markov order of x, i.e. the number of lagged values affecting the current value of x. Default is `lx = 1`.
* `ly`: Markov order of y, i.e. the number of lagged values affecting the current value of y. Default is `ly = 1`.
* `q`: a weighting parameter used to estimate Renyi transfer entropy, parameter is between 0 and 1. For `q = 1`, Renyi transfer entropy converges to Shannon transfer entropy. Default is `q = 0.1`.
* `entropy`: specifies the transfer entropy measure that is estimated, either `'Shannon'` or `'Renyi'`. The first character can be used to specify the type of transfer entropy as well. Default is `entropy = 'Shannon'`.
* `shuffles`: the number of shuffles used to calculate the effective transfer entropy. Default is `shuffles = 100`.
* `cl`: a numeric value (default is number of cores - 1), or a cluster as created by `makeCluster` that can be used by `pbapply`. Specifies the number of cores computations are to distributed over.
* `type`: specifies the type of discretization applied to the observed time series:`'quantiles'`, `'bins'` or `'limits'`. Default is `type = 'quantiles'`.
* `quantiles`: specifies the quantiles of the empirical distribution of the respective time series used for discretization. Default is `quantiles = c(5,95)`.
* `bins`: specifies the number of bins with equal width used for discretization. Default is `bins = NULL`.
* `limits`: specifies the limits on values used for discretization. Default is `limits = NULL`.
* `nboot`: the number of bootstrap replications for each direction of the estimated transfer entropy. Default is `nboot = 300`.
* `burn`: the number of observations that are dropped from the beginning of the bootstrapped Markov chain. Default is `burn = 50`.
* `quiet`: if FALSE (default), the function gives feedback.
* `seed` a seed that seeds the PRNG (will internally just call set.seed), default is `seed = NULL`.

Before we turn to different applications below, we already provide a simple example here, in order to demonstrate how the output of `transfer_entropy()` looks like. Let us consider a linear relationship between two random variables $X$ and $Y$, where $Y$ depends on $X$ with one lag and $X$ is orthogonal to $Y$:

\begin{eqnarray*}
x_t & = & 0.2x_{t-1} + \varepsilon_{x,t}\\
y_t & = & x_{t-1} + \varepsilon_{y,t},
\end{eqnarray*}

with $\varepsilon_{x,t}$ and $\varepsilon_{y,t}$ being normally distributed with a mean of 1 and a variance of 2. In this case, $X$ serves as a predictor for $Y$, but not vice versa. These processes are readily implemented taking, for example, 2500 observations.

```{r, eval=T}
set.seed(12345)
n <- 2500
x <- rep(0, n + 1)
y <- rep(0, n + 1)

for (i in seq(n)) {
     x[i + 1] <- 0.2 * x[i] + rnorm(1, 0, 2)
     y[i + 1] <- x[i] + rnorm(1, 0, 2)
  }
 
x <- x[-1]
y <- y[-1]
```

We estimate Shannon transfer entropy with the defaults for all function arguments and by setting up a cluster.

```{r, eval=T}
set.seed(12345 + 1)
shannon_te <- transfer_entropy(x = x,
                               y = y)
```

While estimating the Shannon transfer entropies, `RTransferEntropy` provides information on the type of transfer entropy that is currently being estimated, the number of cores used for parallel computing, the number of shuffles and bootstrap estimations (for each direction) as well es the length of the time series and the number of removed `NAs` (if any). Progress bars and counters indicate how long estimation and bootstrapping will take. The total time in seconds is displyed after the estimation is done. The output of `transfer_entropy()` (see below) is closely modelled to the typical regression output tables in `R` and summarizes all important information. For each direction of the (possible) information flow, the Shannon transfer entropy is given in the `TE` column. Effective transfer entropy estimates for both direction can be found in the `Eff. TE` column. Standard errors and p-values in the fourth and fifth columns are based on the bootstrap samples, whose quantiles are depicted in the lower part of the ourput table. The `TE` estimates are compared to the quantiles of the bootstrap samples to calculate p-values and to provide an easy to read indication of statistical significance in the last column, according to the definition below the output table. From the output below, we can easily see that there is a significant information flow from $X$ to $Y$ but not vice versa, as simulated.

```{r, eval=T}
shannon_te
```

## Some examples using simulated time series

Consider the above example again, results show a significant information flow from $X$ to $Y$, but not vice versa. Similar conclusions could be drawn from using a vector autoregressive model and testing for Granger causality. However, the main advantage of using transfer entropy is that it is not limited to linear relationships. Consider the following nonlinear relation between $X$ and $Y$, where, again, only $Y$ depends on $X$:
\begin{eqnarray*}
x_t & = & 0.2x_{t-1} + \varepsilon_{x,t}\\
y_t & = & sqrt{\mid x_{t-1}\mid} + \varepsilon_{y,t},
\end{eqnarray*}

with $\varepsilon_{x,t}$ and $\varepsilon_{y,t}$ being standard normally distributed. We simulate these processes, discarding the first 200 observations.

```{r, eval=T}
set.seed(12345)
n <- 2500
x <- rnorm(1, 0, 1)
y <- rnorm(1, 0, 1)

for (i in 2:(n+200)){
  x <- append(x, 0.2 * x[i-1] + rnorm(1, 0, 1))
  y <- append(y, sqrt(abs(x[i-1])) + rnorm(1, 0, 1))
}

x <- x[201:(n+200)]
y <- y[201:(n+200)]
```

Also in this example, we focus on Shannon transfer entropy.

```{r, eval=T}
shannon_te2 <- transfer_entropy(x = x,
                                y = y)

shannon_te2
```

Again, the Shannon Transfer Entropy estimate indicates that there is a significant information flow from $X$ to $Y$, but not in the other direction. 

In the same situation, using a VAR would, however, not reveal any relationship between $X$ and $Y$. Using the package `vars` and one lag (the true lag structure) delivers the following estimates of the VAR(1) for the dependent variable $Y$:

```{r, include=F}
library(vars)
varfit <- VAR(cbind(x, y), p = 1, type = c("const"))
svf <- summary(varfit)
```

```{r, echo=F}
print(svf$varresult$y$coefficients)
```

The VAR cannot detect the nonlinear dependence of $Y$ on $X$ which leads to a parameter estimate x.l1 which is not statistically significant. The autoregressive nature of $X$ is, of course, readily identified. 

As shown above, the question how to determine the quantiles is important as it impacts on the order of magnitude of the transfer entropy. A different approach is provided by R\'{e}nyi transfer entropy which allows to put more weight on the tails in calculating transfer entropy. This is particularly convenient when the distribution is assumed to be more informative in the tails. To illustrate this effect, we simulate data for which the dependence of $Y$ on $X$ changes with the level the innovation.

\begin{eqnarray*}
x_t & = & 0.2x_{t-1} + \varepsilon_{x,t}\\
y_t & = & \begin{cases} \phantom{0.3}x_{t-1} + \varepsilon_{y,t} \quad \text{if } |\varepsilon_{y,t}| > s \\ 0.3x_{t-1} + \varepsilon_{y,t} \quad \text{if } |\varepsilon_{y,t}| < s
\end{cases},
\end{eqnarray*}

with $\varepsilon_{x,t}$ and $\varepsilon_{y,t}$ being standard normally distributed and $s = 2\sigma_y$ and $\sigma_y$ the standard deviation of $\varepsilon_{y,t}$. As before, $X$ serves as a predictor for $Y$, but not vice versa. 

```{r, eval=T}
set.seed(12345)
x <- rnorm(1, 0, 1)
y <- rnorm(1, 0, 1)

for (i in 2:(n + 200)){
  eps_x <- rnorm(1, 0, 1)
  x <- append(x, 0.2 * x[i-1] + eps_x)
  if (eps_x > 2){
    y <- append(y, x[i-1] + rnorm(1, 0, 1))
  }else{
    y <- append(y, .3*x[i-1] + rnorm(1, 0, 1))
  }
}

x <- x[201:(n+200)]
y <- y[201:(n+200)]
```

For comparison, we first estimate Shannon Transfer Entropy using the standard settings which results in the following effective transfer entropy estimates:

```{r, eval=T}
set.seed(12345 + 1)
shannon_te3 <- transfer_entropy(x = x,
                                y = y)

shannon_te3
```

Giving more weights to the tails should reveal that the information flow is largely occuring in the tails. Therefore, we set q=0.3 and invoque the estimatio of R\'{e}nyi transfer entropy as follows:

```{r, eval=F}
set.seed(12345 + 1)
renyi_te <- transfer_entropy(x = x,
                             y = y,
                             entropy = "Renyi",
                             q = 0.3)

renyi_te
```

The result indicates that the tails are indeed more informative. The estimated R\'{e}nyi effective transfer entropy is slightly higher than the Shannon ETE. Again, the information flow from $Y$ to $X$ is correctly identified as being zero.

Since R\'{e}nyi transfer entropy approaches Shannon transfer entropy as $q \rightarrow 1$, we illustrate this property using different values of $q$. Note that this result is illustrated using the normal transfer entropy estimate and not the effective transfer entropy estimates because the latter depend on the shuffling procedure. 

```{r, include=F}
qs <- c(seq(0.1,0.9,0.1),0.99)
ete <- NULL
lab <- NULL
for (i in 1:length(qs)){
  renyi_te <- transfer_entropy(x = x,
                             y = y,
                             lx = 1,
                             ly = 1,
                             entropy = "Renyi",
                             q = qs[i],
                             nboot = 1)
  ete <- append(ete,renyi_te$coef[1,1])
  lab <- append(lab, paste0("q=",qs[i]))
}
names(ete) <- lab
```

As presented above, the Shannon transfer entropy from $X$ to $Y$ is estimated as `r format(round(shannon_te3$coef[1,1],4), nsmall=4)`. Using different values of $q$ we obtain the following results for the R\'{e}nyi transfer entropy.

```{r, echo=F}
print(ete)
```

As can be seen, the value is indeed approaching `r format(round(shannon_te3$coef[1,1],4), nsmall=4)` as $q\rightarrow 1$.

## Application to financial time series

## Speed considerations
If the `transfer_entropy` function is called repeatedly, it makes sense to save the time it takes R to create and stop the cluster in each function-call. We have therefore provided the option to directly specify the cluster using the `cl`-argument.

For example
```{r, eval = F}
transfer_entropy(x1, y1, cl = parallel::detectCores())
transfer_entropy(x2, y2, cl = parallel::detectCores())
transfer_entropy(x3, y3, cl = parallel::detectCores())
transfer_entropy(x4, y4, cl = parallel::detectCores())
...
```

can be simplified to the following, which will reduce the computing time.
```{r, eval = F}
# initialise the cluster
cl <- parallel::makeCluster(parallel::detectCores())
# stop the cluster automatically after execution
on.exit(parallel::stopCluster(cl), add = T)

transfer_entropy(x1, y1, cl = cl)
transfer_entropy(x2, y2, cl = cl)
transfer_entropy(x3, y3, cl = cl)
transfer_entropy(x4, y4, cl = cl)
...
```

Turning the function to `quiet = TRUE` might further help to reduce time when the function is called repeatedly.




## Comparison with existing package

<!-- Vignettes are long form documentation commonly included in packages. Because they are part of the distribution of the package, they need to be as compact as possible. The `html_vignette` output type provides a custom style sheet (and tweaks some options) to ensure that the resulting html is as small as possible. The `html_vignette` format: -->

<!-- - Never uses retina figures -->
<!-- - Has a smaller default figure size -->
<!-- - Uses a custom CSS stylesheet instead of the default Twitter Bootstrap style -->
<!-- Some Examples: -->

<!-- ## Vignette Info -->

<!-- Note the various macros within the `vignette` section of the metadata block above. These are required in order to instruct R how to build the vignette. Note that you should change the `title` field and the `\VignetteIndexEntry` to match the title of your vignette. -->

<!-- ## Styles -->

<!-- The `html_vignette` template includes a basic CSS theme. To override this theme you can specify your own CSS in the document metadata as follows: -->

<!--     output:  -->
<!--       rmarkdown::html_vignette: -->
<!--         css: mystyles.css -->

<!-- ## Figures -->
<!-- We define the `mean_`-function as  (note the `eval=F` in the code: This code will not be run (the function in this case will not be sourced)! It will be displayed only!) -->
<!-- ```{r part1, eval=F} -->
<!-- mean_ <- function(x) sum(x) / length(x) -->
<!-- ``` -->

<!-- The figure sizes have been customised so that you can easily put two images side-by-side.  -->

<!-- ```{r, fig.show='hold'} -->
<!-- plot(1:10) -->
<!-- plot(10:1) -->
<!-- ``` -->

<!-- You can enable figure captions by `fig_caption: yes` in YAML: -->

<!--     output: -->
<!--       rmarkdown::html_vignette: -->
<!--         fig_caption: yes -->

<!-- Then you can use the chunk option `fig.cap = "Your figure caption."` in **knitr**. -->

<!-- ## More Examples -->

<!-- You can write math expressions, e.g. $Y = X\beta + \epsilon$, footnotes^[A footnote here.], and tables, e.g. using `knitr::kable()`. -->

<!-- ```{r, echo=FALSE, results='asis'} -->
<!-- knitr::kable(head(mtcars, 10)) -->
<!-- ``` -->

<!-- Also a quote using `>`: -->

<!-- > "He who gives up [code] safety for [code] speed deserves neither." -->
<!-- ([via](https://twitter.com/hadleywickham/status/504368538874703872)) -->



<!-- # Some David Stuff -->


<!-- Some Examples: -->

<!-- We define the `mean_`-function as  (note the `eval=F` in the code: This code will not be run (the function in this case will not be sourced)! It will be displayed only!) -->
<!-- ```{r part1, eval=F} -->
<!-- mean_ <- function(x) sum(x) / length(x) -->
<!-- ``` -->

<!-- If we want to output something and show the code, we can leave the options empty -->

<!-- ```{r part2} -->
<!-- x <- rnorm(10) -->
<!-- summary(x) -->
<!-- ``` -->

<!-- If we only want to show some output but do not want to show the code, we can use the `echo=F` option -->

<!-- ```{r part3, echo=F} -->
<!-- plot(1:1000, rnorm(1000, sd = 0.01), type = "l", xlab = "Time", ylab = "Return") -->
<!-- ``` -->


<!-- # Equations -->

<!-- To write an equation: -->

<!-- \begin{equation} -->
<!--   x = \sqrt{y + 1} -->
<!-- \end{equation} -->

<!-- or like this  -->

<!-- $$ -->
<!--   x = \sqrt{y + 1} -->
<!-- $$ -->

